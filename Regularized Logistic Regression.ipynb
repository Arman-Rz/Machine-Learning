{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d1461bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a894db6f",
   "metadata": {},
   "source": [
    "# Loading The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ff8c244",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('your_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49398bcd",
   "metadata": {},
   "source": [
    "# Preparing data:\n",
    "1. Seperating features and labels\n",
    "2. Normalizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43e348ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataNormalize(inputData):\n",
    "    mean = np.mean(inputData, axis = 0)\n",
    "    std = np.std(inputData, axis = 0)\n",
    "    normalizedData = (inputData - mean) / std\n",
    "    \n",
    "    return normalizedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b772744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataShuffle(dataSize):\n",
    "    indices = np.arange(dataSize)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f7bfcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataSplit(inputData, outputData):\n",
    "    # Define the split ratio\n",
    "    trainRatio = 0.8\n",
    "    trainSize = int(len(inputData) * trainRatio)\n",
    "    \n",
    "    # Split the data\n",
    "    trainData = inputData[:trainSize]\n",
    "    trainLabel = outputData[:trainSize]\n",
    "    testData = inputData[trainSize:]\n",
    "    testLabel = outputData[trainSize:]\n",
    "    \n",
    "    return trainData, trainLabel, testData, testLabel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8288fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, :-1].values\n",
    "Y = data.iloc[:, -1].values\n",
    "\n",
    "X_normalized = DataNormalize(X)\n",
    "\n",
    "# Shuffle the data\n",
    "indices = DataShuffle(X.shape[0])\n",
    "X_normalized = X_normalized[indices]\n",
    "Y = Y[indices]\n",
    "    \n",
    "    \n",
    "# Split the data\n",
    "trainData, trainLabel, testData, testLabel = DataSplit(X_normalized, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70ffe6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeroOneLoss(trueLabels, predLabels):\n",
    "    return np.sum(trueLabels != predLabels) / len(trueLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c8c5d1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, lambda_param, max_iter):\n",
    "        self.lambda_param = lambda_param\n",
    "        self.max_iter = max_iter\n",
    "        self.weights = None\n",
    "        \n",
    "    def LogisticLossGradient(self, W, X, Y):\n",
    "        z = Y * np.dot(X, W)\n",
    "        gradient = (-Y * X) / (1 + np.exp(z))\n",
    "        return gradient\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        \n",
    "        # Draw a random training example\n",
    "        for t in range(1, self.max_iter + 1):\n",
    "            i = np.random.randint(n_samples)\n",
    "            x_i = X[i]\n",
    "            y_i = Y[i]\n",
    "            \n",
    "            # Compute learning rate for this iteration\n",
    "            eta_t = 1 / (self.lambda_param * t)\n",
    "            \n",
    "            # Compute gradient and update weight\n",
    "            gradient = self.LogisticLossGradient(self.weights, x_i, y_i)\n",
    "            self.weights = (1 - eta_t * self.lambda_param) * self.weights - eta_t * gradient\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.weights is None:\n",
    "            raise ValueError(\"Model has not been trained yet.\")\n",
    "        \n",
    "        z = np.dot(X, self.weights)\n",
    "        probs = self.sigmoid(z)\n",
    "        return np.where(probs >= 0.5, 1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772f243c",
   "metadata": {},
   "source": [
    "# Cross Validation Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f47079f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search using Cross Validateion\n",
    "def crossValScore(X, y, params, k = 5):\n",
    "    fold_size = len(X) // k\n",
    "    accuracies = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        X_test = X[i * fold_size:(i + 1) * fold_size]\n",
    "        y_test = y[i * fold_size:(i + 1) * fold_size]\n",
    "        X_train = np.concatenate((X[:i * fold_size], X[(i + 1) * fold_size:]), axis=0)\n",
    "        y_train = np.concatenate((y[:i * fold_size], y[(i + 1) * fold_size:]), axis=0)\n",
    "        \n",
    "        model = LogisticRegression(lambda_param = params['lambda'], max_iter = params['iters'])\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        predictions = model.predict(X_test)\n",
    "        accuracy = zeroOneLoss(predictions, y_test)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "    return np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6609bfda",
   "metadata": {},
   "source": [
    "# Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "45689ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_127091/1757056210.py:9: RuntimeWarning: overflow encountered in exp\n",
      "  gradient = (-Y * X) / (1 + np.exp(z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda: 0.001, n_iters: 1000, Cross Validation Loss:0.3464\n",
      "lambda: 0.001, n_iters: 1500, Cross Validation Loss:0.3226\n",
      "lambda: 0.001, n_iters: 2000, Cross Validation Loss:0.3196\n",
      "lambda: 0.01, n_iters: 1000, Cross Validation Loss:0.3034\n",
      "lambda: 0.01, n_iters: 1500, Cross Validation Loss:0.3061\n",
      "lambda: 0.01, n_iters: 2000, Cross Validation Loss:0.2983\n",
      "lambda: 0.1, n_iters: 1000, Cross Validation Loss:0.2823\n",
      "lambda: 0.1, n_iters: 1500, Cross Validation Loss:0.2819\n",
      "lambda: 0.1, n_iters: 2000, Cross Validation Loss:0.2794\n",
      "\n",
      "Best Hyperparameters:\n",
      "lambda: 0.1\n",
      "n_iters: 2000\n",
      "Best Cross-Validation loss: 0.2794\n"
     ]
    }
   ],
   "source": [
    "# Tuning Hyperparameters\n",
    "lambda_list = [0.001, 0.01, 0.1]\n",
    "n_iters_list = [1000, 1500, 2000]\n",
    "\n",
    "best_loss = 2\n",
    "best_params = {}\n",
    "for lambdas in lambda_list:\n",
    "    for n_iters in n_iters_list:\n",
    "        params = {'lambda': lambdas,'iters': n_iters}\n",
    "        mean_loss = crossValScore(trainData, trainLabel,params, k = 5)\n",
    "        print(f\"lambda: {lambdas}, n_iters: {n_iters}, Cross Validation Loss:{mean_loss:.4f}\")\n",
    "\n",
    "        if mean_loss < best_loss:\n",
    "            best_loss = mean_loss\n",
    "            best_params = params\n",
    "            \n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(f\"lambda: {best_params['lambda']}\")\n",
    "print(f\"n_iters: {best_params['iters']}\")\n",
    "print(f\"Best Cross-Validation loss: {best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "80a9e017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - Zero-One Loss: 0.2870\n",
      "Iteration 2 - Zero-One Loss: 0.2665\n",
      "Iteration 3 - Zero-One Loss: 0.2765\n",
      "Iteration 4 - Zero-One Loss: 0.2785\n",
      "Iteration 5 - Zero-One Loss: 0.2765\n",
      "Iteration 6 - Zero-One Loss: 0.2875\n",
      "Iteration 7 - Zero-One Loss: 0.2750\n",
      "Iteration 8 - Zero-One Loss: 0.2885\n",
      "Iteration 9 - Zero-One Loss: 0.2710\n",
      "Iteration 10 - Zero-One Loss: 0.2685\n",
      "Average Zero-One Loss: 0.2775\n"
     ]
    }
   ],
   "source": [
    "iterations = 10\n",
    "losses = []\n",
    "for i in range(iterations):\n",
    "\n",
    "    # Shuffle the data\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    X_normalized = X_normalized[indices]\n",
    "    Y = Y[indices]\n",
    "\n",
    "    # Define the split ratio\n",
    "    trainRatio = 0.8\n",
    "    trainSize = int(len(X_normalized) * trainRatio)\n",
    "\n",
    "    # Split the data\n",
    "    trainData = X_normalized[:trainSize]\n",
    "    trainLabel = Y[:trainSize]\n",
    "\n",
    "    testData = X_normalized[trainSize:]\n",
    "    testLabel = Y[trainSize:]\n",
    "\n",
    "    model = LogisticRegression(lambda_param = best_params['lambda'], max_iter = best_params['iters'])\n",
    "        \n",
    "    model.fit(trainData, trainLabel)\n",
    "\n",
    "    predictions = model.predict(testData)\n",
    "    \n",
    "    loss = zeroOneLoss(testLabel, predictions)\n",
    "    print(f\"Iteration {i + 1} - Zero-One Loss: {loss:.4f}\")\n",
    "    losses.append(loss)\n",
    "\n",
    "averageLoss = np.mean(losses)\n",
    "print(f\"Average Zero-One Loss: {averageLoss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "32801b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data.columns[:-1]\n",
    "output = data.columns[-1]\n",
    "\n",
    "for feature in features:\n",
    "    data[feature + '_squared'] = data[feature] ** 2\n",
    "    \n",
    "columns_order = [col for col in data.columns if col != output] + [output]\n",
    "data = data[columns_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e17f2ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, :-1].values\n",
    "Y = data.iloc[:, -1].values\n",
    "\n",
    "mean = np.mean(X, axis = 0)\n",
    "std = np.std(X, axis = 0)\n",
    "\n",
    "X_normalized = (X - mean) / std\n",
    "\n",
    "# Split the data\n",
    "trainData, trainLabel, testData, testLabel = DataSplit(X_normalized, Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
