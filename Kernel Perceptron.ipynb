{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "052b483b-fcac-4855-8c76-e75d9614f0cb",
   "metadata": {},
   "source": [
    "### In this project, we use the kernelized version of Perceptron algorithm to create our prediction fot the given dataset. Due to the high time complexity of this algorithm, the process is time consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fc7273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a894db6f",
   "metadata": {},
   "source": [
    "# Loading The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ff8c244",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('your_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49398bcd",
   "metadata": {},
   "source": [
    "# Preparing data:\n",
    "1. Seperating features and labels\n",
    "2. Normalizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b772744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataNormalize(inputData):\n",
    "    mean = np.mean(inputData, axis = 0)\n",
    "    std = np.std(inputData, axis = 0)\n",
    "    normalizedData = (inputData - mean) / std\n",
    "    \n",
    "    return normalizedData\n",
    "\n",
    "def DataShuffle(dataSize):\n",
    "    indices = np.arange(dataSize)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    return indices\n",
    "\n",
    "def DataSplit(inputData, outputData):\n",
    "    # Define the split ratio\n",
    "    trainRatio = 0.8\n",
    "    trainSize = int(len(inputData) * trainRatio)\n",
    "    \n",
    "    # Split the data\n",
    "    trainData = inputData[:trainSize]\n",
    "    trainLabel = outputData[:trainSize]\n",
    "    \n",
    "    testData = inputData[trainSize:]\n",
    "    testLabel = outputData[trainSize:]\n",
    "    \n",
    "    return trainData, trainLabel, testData, testLabel\n",
    "\n",
    "def zeroOneLoss(trueLabels, predLabels):\n",
    "    return np.sum(trueLabels != predLabels) / len(trueLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8288fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, :-1].values\n",
    "Y = data.iloc[:, -1].values\n",
    "\n",
    "X_normalized = DataNormalize(X)\n",
    "\n",
    "# Shuffle the data\n",
    "indices = DataShuffle(X.shape[0])\n",
    "X_normalized = X_normalized[indices]\n",
    "Y = Y[indices]\n",
    "    \n",
    "    \n",
    "# Split the data\n",
    "trainData, trainLabel, testData, testLabel = DataSplit(X_normalized, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3038de92",
   "metadata": {},
   "source": [
    "# Kernel Perceptron Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e387949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelPerceptron:\n",
    "    def __init__(self, kernel='gaussian', degree=3, gamma=1.0, coef0=1.0, max_iter=100):\n",
    "        self.kernel = kernel\n",
    "        self.degree = degree\n",
    "        self.gamma = gamma\n",
    "        self.coef0 = coef0\n",
    "        self.max_iter = max_iter\n",
    "        self.support_vectors = []  # To store indices of support vectors\n",
    "        self.alphas = []\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.kernel_matrix = None\n",
    "\n",
    "    def _gaussian_kernel(self, X, Y):\n",
    "        return np.exp(-self.gamma * np.linalg.norm(X[:, np.newaxis] - Y, axis=2) ** 2)\n",
    "\n",
    "    def _polynomial_kernel(self, X, Y):\n",
    "        return (self.gamma * np.dot(X, Y.T) + self.coef0) ** self.degree\n",
    "\n",
    "    def _compute_kernel(self, X, Y):\n",
    "        if self.kernel == 'gaussian':\n",
    "            return self._gaussian_kernel(X, Y)\n",
    "        elif self.kernel == 'polynomial':\n",
    "            return self._polynomial_kernel(X, Y)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown kernel type: {self.kernel}\")\n",
    "\n",
    "    def _precompute_kernel_matrix(self, X):\n",
    "        self.kernel_matrix = self._compute_kernel(X, X)\n",
    "\n",
    "    def _compute_single_prediction(self, x):\n",
    "        if not self.support_vectors:\n",
    "            return -1\n",
    "        else:\n",
    "            sv_indices = np.array(self.support_vectors)\n",
    "            X_sv = self.X_train[sv_indices]\n",
    "            K = self._compute_kernel(X_sv, x[np.newaxis, :])\n",
    "            return np.sign(np.dot(self.alphas, self.y_train[sv_indices] * K.ravel()))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        self._precompute_kernel_matrix(X)\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            no_errors = True\n",
    "            for t in range(n_samples):\n",
    "                prediction = self._compute_single_prediction(X[t])\n",
    "                if prediction != y[t]:\n",
    "                    self.support_vectors.append(t)\n",
    "                    self.alphas.append(1)  # Use alpha for weight adjustment\n",
    "                    no_errors = False\n",
    "            \n",
    "            if no_errors:\n",
    "                print(f\"Stopping early at iteration {iteration}\")\n",
    "                break\n",
    "\n",
    "    def predict(self, X):\n",
    "        if not self.support_vectors:\n",
    "            return np.zeros(X.shape[0])\n",
    "        \n",
    "        predictions = []\n",
    "        for t in range(X.shape[0]):\n",
    "            prediction = self._compute_single_prediction(X[t])\n",
    "            predictions.append(prediction)\n",
    "        \n",
    "        return np.array(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b20b045",
   "metadata": {},
   "source": [
    "# Cross Validation Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35bbb217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search using Cross Validateion\n",
    "def crossValScore(X, y, params, k = 5):\n",
    "    fold_size = len(X) // k\n",
    "    accuracies = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        \n",
    "        X_test = X[i * fold_size:(i + 1) * fold_size]\n",
    "        y_test = y[i * fold_size:(i + 1) * fold_size]\n",
    "        X_train = np.concatenate((X[:i * fold_size], X[(i + 1) * fold_size:]), axis=0)\n",
    "        y_train = np.concatenate((y[:i * fold_size], y[(i + 1) * fold_size:]), axis=0)\n",
    "\n",
    "        kernelPerceptron = KernelPerceptron(kernel = params['kernel'], degree =  params['degree'], gamma = params['gamma'], coef0 = params['coef0'], max_iter =  params['max_iter'])\n",
    "        \n",
    "        kernelPerceptron.fit(X_train, y_train)\n",
    "        \n",
    "        predictions = kernelPerceptron.predict(X_test)\n",
    "        accuracy = zeroOneLoss(predictions, y_test)\n",
    "        \n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "    return np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db61d15e",
   "metadata": {},
   "source": [
    "# Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2725a3c-7e6a-459a-9732-2236535e82ac",
   "metadata": {},
   "source": [
    "### In order to speed up the process, we use parallel programming and use all existing CPU threads for each set of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "decc9726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Tuning Hyperparameters for Gaussian Kernel: \n",
      "\n",
      "gamma: 0.1, n_iters: 20, Cross Validation Loss:0.07629768605378362\n",
      "gamma: 0.1, n_iters: 30, Cross Validation Loss:0.07191994996873045\n",
      "gamma: 0.1, n_iters: 40, Cross Validation Loss:0.06879299562226392\n",
      "gamma: 1, n_iters: 20, Cross Validation Loss:0.17073170731707318\n",
      "gamma: 1, n_iters: 30, Cross Validation Loss:0.17073170731707318\n",
      "gamma: 1, n_iters: 40, Cross Validation Loss:0.17073170731707318\n",
      "gamma: 10, n_iters: 20, Cross Validation Loss:0.23076923076923075\n",
      "gamma: 10, n_iters: 30, Cross Validation Loss:0.23076923076923075\n",
      "gamma: 10, n_iters: 40, Cross Validation Loss:0.23076923076923075\n",
      "\n",
      "Best Hyperparameters:\n",
      "\n",
      "Gaussian: \n",
      "\t gamma: 0.1\n",
      "\t max_iters: 40\n",
      "\t Best Cross-Validation loss: 0.06879299562226392\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Gaussian kernel parameters:\n",
    "gamma_values = [0.1, 1, 10]\n",
    "max_iter_values = [20, 30, 40]\n",
    "\n",
    "# Polynomial kernel parameters\n",
    "degree_values = [2, 3, 4]\n",
    "coef0_values = [1, 5, 10]\n",
    "\n",
    "inputData = trainData[: int(0.2 * len(trainData))]\n",
    "outputData = trainLabel[: int(0.2 * len(trainLabel))]\n",
    "\n",
    "def evaluate_params(params):\n",
    "    mean_loss = crossValScore(inputData, outputData, params, k=3)\n",
    "    return params, mean_loss\n",
    "\n",
    "# Gaussian kernel\n",
    "print(\"Tuning Hyperparameters for Gaussian Kernel: \\n\")\n",
    "best_gaussian_loss = 2\n",
    "best_gaussian_params = {}\n",
    "\n",
    "# Generate parameter combinations for Gaussian kernel\n",
    "gaussian_params_list = [{'kernel': 'gaussian', 'degree': 3, 'gamma': gamma, 'coef0': 1.0, 'max_iter': iters}\n",
    "                        for gamma in gamma_values for iters in max_iter_values]\n",
    "\n",
    "# Evaluate all parameter combinations in parallel\n",
    "results = Parallel(n_jobs=-1)(delayed(evaluate_params)(params) for params in gaussian_params_list)\n",
    "\n",
    "for params, mean_loss in results:\n",
    "    print(f\"gamma: {params['gamma']}, n_iters: {params['max_iter']}, Cross Validation Loss:{mean_loss}\")\n",
    "    if mean_loss < best_gaussian_loss:\n",
    "        best_gaussian_loss = mean_loss\n",
    "        best_gaussian_params = params\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\\n\")\n",
    "print(\"Gaussian: \")\n",
    "print(f\"\\t gamma: {best_gaussian_params['gamma']}\")\n",
    "print(f\"\\t max_iters: {best_gaussian_params['max_iter']}\")\n",
    "print(f\"\\t Best Cross-Validation loss: {best_gaussian_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0546700f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0395"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = best_gaussian_params\n",
    "kernelPerceptron = KernelPerceptron(kernel = params['kernel'], degree = params['degree'], gamma = params['gamma'], coef0 = params['coef0'], max_iter =  params['max_iter'])\n",
    "kernelPerceptron.fit(trainData, trainLabel)\n",
    "preds = kernelPerceptron.predict(testData)\n",
    "zeroOneLoss(preds, testLabel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
